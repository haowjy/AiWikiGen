{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at OpenMatch/cocodr-large-msmarco were not used when initializing BertForMaskedLM: ['norm.bias', 'loss.count_cat', 'embeddingHead.weight', 'loss.sum_losses', 'classifier.weight', 'classifier.bias', 'embeddingHead.bias', 'norm.weight', 'loss.h_fun']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at OpenMatch/cocodr-large-msmarco and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"OpenMatch/cocodr-large-msmarco\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"OpenMatch/cocodr-large-msmarco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index import download_loader, GPTSimpleVectorIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"creds.json\", \"r\") as f:\n",
    "    creds = json.load(f)\n",
    "\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = creds['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from gpt_index import download_loader\n",
    "\n",
    "PDFReader = download_loader(\"PDFReader\")\n",
    "\n",
    "loader = PDFReader()\n",
    "documents = loader.load_data(file=Path('../data/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 543794 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTSimpleVectorIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.save_to_disk('index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = GPTSimpleVectorIndex.load_from_disk('index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def args_to_dict(**kwargs):\n",
    "    return {**kwargs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To set up a support vector machine equation, one must first define a kernel function k(x,x') and a Lagrange multiplier an for each data point xn. The equation is then expressed in terms of the kernel function as y(x) = N∑n=1(an - ̂an)k(x,xn) + b, where b is a bias parameter. The constraints for the Lagrange multipliers are an ≥ 0, ̂an ≥ 0, an ≤ C, and ̂an ≤ C, where C is a parameter that controls the complexity of the model. Additionally, the Karush-Kuhn-Tucker (KKT) conditions must be satisfied, which state that at the solution the product of the dual variables and the constraints must vanish. This is expressed as an(ε1 + ξn + yn - tn) = 0, ̂an(ε1 + ̂ξn - yn + tn) = 0, (C - an)ξn = 0, and (C - ̂an)̂ξn = 0. Furthermore, a prior distribution over the parameter vector w and a zero\n"
     ]
    }
   ],
   "source": [
    "response = index.query(query_str=\"How do you set up support vector machine equations?\", similarity_top_k=5)\n",
    "# 1m 41.3s\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses.append({\n",
    "    \"query\": args_to_dict(query_str=\"How do you set up support vector machine equations?\", similarity_top_k=5),\n",
    "    \"response\": response\n",
    " })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 4254 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 10 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To set up support vector machine equations, one must first define a linear model of the form y(x) = wTφ(x), where w is the weight parameter and φ(x) is the nonlinear basis function. The conditional distribution for a real-valued target variable t, given an input vector x, is then given by p(t|x,w,β) = N(t|y(x),β-1). The prior distribution over the parameter vector w is a zero-mean Gaussian prior with a separate hyperparameter αi for each of the weight parameters wi, as expressed in equation (7.80). Finally, predictions are expressed as linear combinations of kernel functions that are centered on training data points and that are required to be positive definite.\n"
     ]
    }
   ],
   "source": [
    "response2 = index.query(query_str=\"How do you set up support vector machine equations?\", similarity_top_k=1)\n",
    "print(response2)\n",
    "# 16.5s\n",
    "# INFO:root:> [query] Total LLM token usage: 4254 tokens\n",
    "# INFO:root:> [query] Total embedding token usage: 10 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses.append({\n",
    "    \"query\": args_to_dict(query_str=\"How do you set up support vector machine equations?\", similarity_top_k=1),\n",
    "    \"response\": response2\n",
    " })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('all_responses.pkl', 'wb') as f:\n",
    "    pickle.dump(all_responses, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikigen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0a43d17ba3ed57e07e41f084ec0592580aff54bbac62f33d320225f071b8718"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
